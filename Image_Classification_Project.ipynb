{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook_homework1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyUJAJbuUyjA"
      },
      "source": [
        "#Table of content\n",
        "1.   Connect drive\n",
        "2.   Libraries\n",
        "3.   Prepare the training data\n",
        "4.   CNN the classical way\n",
        "  1.   Data augmentation\n",
        "  2.   Dealing with an imbalanced data\n",
        "  3.   In-House Neural Network\n",
        "5.   Transfer Learning\n",
        "  1.   Data preparation\n",
        "  1.   VGG16\n",
        "  2.   InceptionV3\n",
        "  3.   Inception model fine tuning : Final Model\n",
        "6.   Analysis of the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0B-FyIE3Lhq"
      },
      "source": [
        "## Connect drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh7nJkDf3rAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709fb017-2efa-4602-e0e9-ff82685d4281"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae-5gQN33uRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52113a6a-1269-46b8-be20-2918c83f8e6a"
      },
      "source": [
        "%cd /gdrive/MyDrive/AN2DL_Challenge1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/AN2DL_Challenge1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4dfivm83QYo"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGWCzwOd3w9y"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7xcu1US32rT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa5773a-c4cc-4b9e-8caf-a17cfc0aedef"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from PIL import Image\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1riU11ay4Mq0"
      },
      "source": [
        "# Random seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XswyDerl35IT"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcQiaS-c35mi"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtYWWqG13TbT"
      },
      "source": [
        "## Prepare the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43TSySPQ4C8i"
      },
      "source": [
        "#!unzip dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfJIK81k4GQF"
      },
      "source": [
        "source = 'training'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jfxVuC34H7q"
      },
      "source": [
        "#the name of the classes\n",
        "labels = ['Apple','Blueberry','Cherry','Corn','Grape','Orange','Peach','Pepper','Potato','Raspberry','Soybean','Squash','Strawberry','Tomato']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J-sRT6p7KDg"
      },
      "source": [
        "## CNN the classical way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXpfSXXx3XFK"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PxTnjIYYrXw"
      },
      "source": [
        "test_source = 'test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygDuS1oG4V3j"
      },
      "source": [
        "# ImageDataGenerator\n",
        "# ------------------\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "apply_data_augmentation = True  \n",
        "\n",
        "# Create training ImageDataGenerator object\n",
        "if apply_data_augmentation:\n",
        "    train_data_gen = ImageDataGenerator(\n",
        "                                        rotation_range=10,\n",
        "                                        width_shift_range=10,\n",
        "                                        height_shift_range=10,\n",
        "                                        zoom_range=0.3,\n",
        "                                        horizontal_flip=True,\n",
        "                                        vertical_flip=False,\n",
        "                                        fill_mode='constant',\n",
        "                                        cval=0,\n",
        "                                        rescale=1./255,\n",
        "                                        preprocessing_function=None,\n",
        "                                        data_format='channels_last',\n",
        "                                        validation_split=0.2,    \n",
        "                                       )\n",
        "else:\n",
        "    train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#here I create the validation data generator\n",
        "valid_data_gen = ImageDataGenerator(rescale=1./255,validation_split=0.2)\n",
        "\n",
        "test_data_gen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re3aCS3a4ZsK"
      },
      "source": [
        "# Create generators to read images from dataset directory\n",
        "# -------------------------------------------------------\n",
        "\n",
        "# Batch size\n",
        "bs = 32\n",
        "\n",
        "# Image shape\n",
        "img_h = 256   #height of the images\n",
        "img_w = 256   #width of the images\n",
        "\n",
        "num_classes = 14\n",
        "\n",
        "classes = labels\n",
        "\n",
        "#Training\n",
        "train_gen = train_data_gen.flow_from_directory(source,\n",
        "                                               batch_size=bs,\n",
        "                                               classes=classes,\n",
        "                                               class_mode='categorical',\n",
        "                                               shuffle=True, #it's nice to have them shuffled\n",
        "                                               subset='training',\n",
        "                                               seed=seed)  \n",
        "\n",
        "# Validation \n",
        "valid_gen = valid_data_gen.flow_from_directory(source,\n",
        "                                                batch_size=bs,\n",
        "                                                classes=classes,\n",
        "                                                class_mode='categorical',\n",
        "                                                shuffle=True, \n",
        "                                                subset='validation',\n",
        "                                                seed=seed)\n",
        "\n",
        "test_gen = test_data_gen.flow_from_directory(test_source, #different from the other\n",
        "                                                batch_size=1,\n",
        "                                                classes=classes,\n",
        "                                                class_mode='categorical',\n",
        "                                                shuffle=False, #to be able to print the confusion matrix \n",
        "                                                subset='training',\n",
        "                                                seed=seed) # set as validation data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqrcrRbh63ZW"
      },
      "source": [
        "### Dealing with an imbalanced dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dNDcSAUVTfj"
      },
      "source": [
        "# Compute the class weights in order to balance loss during training\n",
        "counter = Counter(train_gen.classes)                          \n",
        "max_val = float(max(counter.values()))       \n",
        "class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}   \n",
        "print(class_weights)\n",
        "\n",
        "# This will help, conjugated with a slight oversampling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr122hv0X8iZ"
      },
      "source": [
        "### In-House Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYHek69d7QUu"
      },
      "source": [
        "input_shape = (256, 256, 3)\n",
        "epochs = 200\n",
        "\n",
        "def build_model(input_shape):\n",
        "\n",
        "    # Build the neural network layer by layer\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "    resizing_layer = tfkl.Resizing(64, 64, interpolation=\"bilinear\", crop_to_aspect_ratio=False)(input_layer)\n",
        "\n",
        "    conv1 = tfkl.Conv2D(\n",
        "        filters=16,\n",
        "        kernel_size=(3, 3),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        #kernel_regularizer = 'l2'\n",
        "    )(resizing_layer)\n",
        "\n",
        "    bn_layer1 = tfkl.BatchNormalization()(conv1)\n",
        "\n",
        "    pool1 = tfkl.MaxPooling2D(\n",
        "        pool_size = (2, 2)\n",
        "    )(bn_layer1)\n",
        "\n",
        "    conv2 = tfkl.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(3, 3),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        #kernel_regularizer = 'l2'\n",
        "    )(pool1)\n",
        "\n",
        "    bn_layer2 = tfkl.BatchNormalization()(conv2)\n",
        "\n",
        "\n",
        "    pool2 = tfkl.MaxPooling2D(\n",
        "        pool_size = (2, 2)\n",
        "    )(bn_layer2)\n",
        "\n",
        "    conv3 = tfkl.Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        #kernel_regularizer = 'l2'\n",
        "    )(pool2)\n",
        "\n",
        "    bn_layer3 = tfkl.BatchNormalization()(conv3)\n",
        "\n",
        "    pool3 = tfkl.MaxPooling2D(\n",
        "        pool_size = (2, 2)\n",
        "    )(bn_layer3)\n",
        "\n",
        "    conv4 = tfkl.Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        #kernel_regularizer = 'l2'\n",
        "    )(pool3)\n",
        "\n",
        "    bn_layer4 = tfkl.BatchNormalization()(conv4)\n",
        "\n",
        "    pool4 = tfkl.MaxPooling2D(\n",
        "        pool_size = (2, 2)\n",
        "    )(bn_layer4)\n",
        "\n",
        "    conv5 = tfkl.Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        #kernel_regularizer = 'l2'\n",
        "    )(pool4)\n",
        "\n",
        "    bn_layer5 = tfkl.BatchNormalization()(conv5)\n",
        "\n",
        "    pool5 = tfkl.MaxPooling2D(\n",
        "        pool_size = (2, 2)\n",
        "    )(bn_layer5)\n",
        "\n",
        "    flattening_layer = tfkl.Flatten(name='Flatten')(pool5)\n",
        "    flattening_layer = tfkl.Dropout(0.30, seed=seed)(flattening_layer)\n",
        "    classifier_layer = tfkl.Dense(units=512, name='Classifier', activation='relu')(flattening_layer)\n",
        "    classifier_layer = tfkl.Dropout(0.30, seed=seed)(classifier_layer)\n",
        "    output_layer = tfkl.Dense(units=14, activation='softmax', name='Output')(classifier_layer)\n",
        "\n",
        "    # Connect input and output through the Model class\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
        "\n",
        "    # Return the model\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCPFajBY7Zsr"
      },
      "source": [
        "model = build_model(input_shape)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PLjgRBl7dZz"
      },
      "source": [
        "patience = 15\n",
        "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=patience, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x = train_gen,\n",
        "    validation_data = valid_gen,\n",
        "    epochs = 250,\n",
        "    shuffle = True,\n",
        "    class_weight = class_weights,\n",
        "    callbacks = [early_stopping]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChFLADU57mHs"
      },
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "# Save model, with the date in its name for no confusion\n",
        "model.save(\"model_saved_\" + now)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt6qnh5E7qWg"
      },
      "source": [
        "#In case we want to continue the training of an old model\n",
        "#model = tfk.models.load_model('D:/ANNDL/model_saved_Nov25_10-01-50')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK5EaZ92VFI3"
      },
      "source": [
        "## Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw0sIUEwYXD_"
      },
      "source": [
        "### Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WprGD2gWYSX6"
      },
      "source": [
        "# ImageDataGenerator\n",
        "\n",
        "apply_data_augmentation = True  \n",
        "\n",
        "# Create training ImageDataGenerator object\n",
        "if apply_data_augmentation:\n",
        "    train_data_gen = ImageDataGenerator(\n",
        "                                        rotation_range=10,\n",
        "                                        width_shift_range=10,\n",
        "                                        height_shift_range=10,\n",
        "                                        zoom_range=0.3,\n",
        "                                        horizontal_flip=True,\n",
        "                                        vertical_flip=False,\n",
        "                                        fill_mode='constant',\n",
        "                                        cval=0,\n",
        "                                        rescale=1./255,\n",
        "                                        preprocessing_function=None,\n",
        "                                        data_format='channels_last',\n",
        "                                        validation_split=0.2,    \n",
        "                                       )\n",
        "else:\n",
        "    train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#Create the validation data generator\n",
        "valid_data_gen = ImageDataGenerator(rescale=1./255,validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLfedfTqZaQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d245712-7bc9-47a7-f31e-1303d47178cc"
      },
      "source": [
        "# Batch size\n",
        "bs = 32\n",
        "\n",
        "# Image shape\n",
        "img_h = 256   #height\n",
        "img_w = 256   #width\n",
        "\n",
        "num_classes = 14\n",
        "\n",
        "classes = labels\n",
        "\n",
        "#Training\n",
        "train_gen = train_data_gen.flow_from_directory(source,\n",
        "                                               batch_size=bs,\n",
        "                                               classes=classes,\n",
        "                                               class_mode='categorical',\n",
        "                                               shuffle=True, #it's nice to have them shuffled\n",
        "                                               subset='training',\n",
        "                                               seed=seed)  \n",
        "\n",
        "# Validation \n",
        "valid_gen = valid_data_gen.flow_from_directory(source,\n",
        "                                                batch_size=bs,\n",
        "                                                classes=classes,\n",
        "                                                class_mode='categorical',\n",
        "                                                shuffle=True, \n",
        "                                                subset='validation',\n",
        "                                                seed=seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14189 images belonging to 14 classes.\n",
            "Found 3539 images belonging to 14 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRFN8bJYVV63"
      },
      "source": [
        "# Create Dataset objects\n",
        "# ----------------------\n",
        "\n",
        "# Training\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "# Validation\n",
        "# ----------\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRlBv6CDb8zP"
      },
      "source": [
        "# Compute the class weights\n",
        "counter = Counter(train_gen.classes)                          \n",
        "max_val = float(max(counter.values()))       \n",
        "class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}   \n",
        "print(class_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLw99wue1EP3"
      },
      "source": [
        "### VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUVWfIz81H7R"
      },
      "source": [
        "# VGG16\n",
        "Vgg16 = tf.keras.applications.VGG16(weights='imagenet', \n",
        "                                include_top=False, \n",
        "                                input_shape=(img_h, img_w,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGapv90f1etX"
      },
      "source": [
        "# model for Transfer Learning done with VGG16\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(Vgg16)\n",
        "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pnGEEvt1hVb"
      },
      "source": [
        "# Loss\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# learning rate\n",
        "lr = 2e-5\n",
        "#optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "# Validation metrics\n",
        "metrics = ['accuracy']\n",
        "#callbacks\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,restore_best_weights=True)\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANiRtaOcZ3av"
      },
      "source": [
        "# I usually stop the training manually because I don't want colab to crash and lose everything\n",
        "model.fit(x=train_dataset,\n",
        "          epochs=30,\n",
        "          steps_per_epoch=len(train_gen),\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(valid_gen),\n",
        "          class_weight=class_weights,\n",
        "          callbacks = es_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfbW95tc1ljV"
      },
      "source": [
        "model.save(\"classification_experiments/vggattempt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8_hT00O3f9h"
      },
      "source": [
        "### InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te2jx1lLVaeW"
      },
      "source": [
        "# InceptionV3\n",
        "inception = tf.keras.applications.InceptionV3(weights='imagenet', \n",
        "                                include_top=False, \n",
        "                                input_shape=(img_h, img_w,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQIAh8ZRJqic"
      },
      "source": [
        "# model for Transfer Learning done with Inception\n",
        "model1 = tf.keras.models.Sequential()\n",
        "model1.add(inception)\n",
        "model1.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "model1.add(tf.keras.layers.Dropout(0.3))\n",
        "model1.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model1.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3xhOf7GV8y2"
      },
      "source": [
        "# Loss\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# learning rate\n",
        "lr = 2e-5\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "# -------------------\n",
        "\n",
        "# Validation metrics\n",
        "# ------------------\n",
        "\n",
        "metrics = ['accuracy']\n",
        "# ------------------\n",
        "\n",
        "# Compile Model\n",
        "model1.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "#callbacks\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ6NDHa1KZlT"
      },
      "source": [
        "hist1 = model1.fit(x=train_dataset,\n",
        "          epochs=30,\n",
        "          steps_per_epoch=len(train_gen),\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(valid_gen),\n",
        "          class_weight=class_weights,callbacks = es_callback).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlJKC85EWDiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac0a523-b203-4e03-ebeb-102bc5e72474"
      },
      "source": [
        "model1.save(\"classification_experiments/inceptionattempt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: classification_experiments/inceptionattempt/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7kvafzNbU8t"
      },
      "source": [
        "### Inception model fine tuning : Final Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvBFA7Sg9Zkl"
      },
      "source": [
        "del model1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj-6_sueJz1u"
      },
      "source": [
        "# Re-load the model after transfer learning\n",
        "ft_model = tfk.models.load_model('classification_experiments/inceptionattempt')\n",
        "ft_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_ltr31K7Ui"
      },
      "source": [
        "# Set all Inception layers to True\n",
        "ft_model.get_layer('inception_v3').trainable = True\n",
        "for i, layer in enumerate(ft_model.get_layer('inception_v3').layers):\n",
        "   print(i, layer.name, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLiSLShsK_Uq"
      },
      "source": [
        "# Freeze first 285 layers\n",
        "for i, layer in enumerate(ft_model.get_layer('inception_v3').layers[:285]):\n",
        "  layer.trainable=False\n",
        "for i, layer in enumerate(ft_model.get_layer('inception_v3').layers):\n",
        "   print(i, layer.name, layer.trainable)\n",
        "ft_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYQ3G3bUI_5_"
      },
      "source": [
        "# Compile the model\n",
        "ft_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-6Tw_zrLC01"
      },
      "source": [
        "# Fine-tune the model\n",
        "ft_history = ft_model.fit(x=train_dataset,\n",
        "          epochs=30,\n",
        "          steps_per_epoch=len(train_gen),\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(valid_gen),\n",
        "          class_weight=class_weights,callbacks = es_callback).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8KelEagP2u9",
        "outputId": "4b104f00-c025-4a97-a06e-b49d16723594"
      },
      "source": [
        "ft_model.save('classification_experiments/FineTuningModel')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: classification_experiments/FineTuningModel/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LzHgzen7x9w"
      },
      "source": [
        "## Analysis of the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlHs-IxS71kL"
      },
      "source": [
        "# Plot the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, top_k_accuracy_score, balanced_accuracy_score\n",
        "\n",
        "y_pred = model.predict(test_gen)\n",
        "y_pred_m = np.argmax(y_pred, axis=1)\n",
        "\n",
        "cm = confusion_matrix(test_gen.classes, y_pred_m, normalize='true')\n",
        "b_acc = balanced_accuracy_score(test_gen.classes, y_pred_m)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "im = ax.imshow(cm)\n",
        "ax.set_title(\"Matrice de confusion\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "print('Balanced accuracy = ', b_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}